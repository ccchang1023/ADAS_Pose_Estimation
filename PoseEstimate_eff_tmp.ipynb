{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "import seaborn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "SWITCH_LOSS_EPOCH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ImageId</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ID_8a6e65317</td>\n",
       "      <td>16 0.254839 -2.57534 -3.10256 7.96539 3.20066 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       ImageId                                   PredictionString\n",
       "0           0  ID_8a6e65317  16 0.254839 -2.57534 -3.10256 7.96539 3.20066 ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"./dataset/\"\n",
    "train_pd = pd.read_csv(PATH + 'train_remove.csv')\n",
    "# train_pd = pd.read_csv(PATH + 'train.csv')\n",
    "test_pd = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "# From camera.zip\n",
    "camera_matrix = np.array([[2304.5479, 0,  1686.2379],\n",
    "                          [0, 2305.8757, 1354.9849],\n",
    "                          [0, 0, 1]], dtype=np.float32)\n",
    "camera_matrix_inv = np.linalg.inv(camera_matrix)\n",
    "train_pd.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(points_df) 49607\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>yaw</th>\n",
       "      <th>pitch</th>\n",
       "      <th>roll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.96539</td>\n",
       "      <td>3.20066</td>\n",
       "      <td>11.0225</td>\n",
       "      <td>0.254839</td>\n",
       "      <td>-2.57534</td>\n",
       "      <td>-3.10256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x        y        z       yaw    pitch     roll\n",
       "0  7.96539  3.20066  11.0225  0.254839 -2.57534 -3.10256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str2coords(s, names=['id', 'yaw', 'pitch', 'roll', 'x', 'y', 'z']):\n",
    "    coords = []\n",
    "    for l in np.array(s.split()).reshape([-1, 7]):\n",
    "        coords.append(dict(zip(names, l.astype('float'))))\n",
    "        coords[-1]['id'] = int(coords[-1]['id'])\n",
    "    return coords\n",
    "\n",
    "# lens = [len(str2coords(s)) for s in train['PredictionString']]\n",
    "# plt.figure(figsize=(15,6))\n",
    "# seaborn.countplot(lens);\n",
    "# plt.xlabel('Number of cars in image')\n",
    "\n",
    "points_df = pd.DataFrame()\n",
    "for col in ['x', 'y', 'z', 'yaw', 'pitch', 'roll']:\n",
    "    arr = []\n",
    "    for ps in train_pd['PredictionString']:\n",
    "        coords = str2coords(ps)\n",
    "        arr += [c[col] for c in coords]\n",
    "    points_df[col] = arr\n",
    "print('len(points_df)', len(points_df))\n",
    "points_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_coords(s):\n",
    "    coords = str2coords(s)\n",
    "    xs = [c['x'] for c in coords]\n",
    "    ys = [c['y'] for c in coords]\n",
    "    zs = [c['z'] for c in coords]\n",
    "    P = np.array(list(zip(xs, ys, zs))).T\n",
    "    img_p = np.dot(camera_matrix, P).T\n",
    "    img_p[:, 0] /= img_p[:, 2]\n",
    "    img_p[:, 1] /= img_p[:, 2]\n",
    "    img_xs = img_p[:, 0]\n",
    "    img_ys = img_p[:, 1]\n",
    "    img_zs = img_p[:, 2] # z = Distance from the camera\n",
    "    return img_xs, img_ys\n",
    "\n",
    "# train_root = \"./dataset/train_images/\"\n",
    "# for idx,_ in enumerate(os.listdir(train_root)):\n",
    "#     if idx<1:\n",
    "#         name = train['ImageId'][idx]\n",
    "#         img = Image.open(\"{}/{}.jpg\".format(train_root,name))\n",
    "#         coordinate = get_img_coords(train['PredictionString'][idx])\n",
    "# #         print(coordinate)\n",
    "#         fig,axes = plt.subplots(1,1,figsize=(7,7))\n",
    "#         axes.imshow(img)\n",
    "#         axes.scatter(*coordinate,color='red',s=30)\n",
    "#         plt.pause(.1)\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given x,z -> get y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE without x: 0.6557414214081664\n",
      "MAE with x: 0.4484400166066845\n",
      "\n",
      "dy/dx = 0.051\n",
      "dy/dz = 0.169\n"
     ]
    }
   ],
   "source": [
    "zy_slope = LinearRegression()\n",
    "X = points_df[['z']]\n",
    "y = points_df['y']\n",
    "zy_slope.fit(X, y)\n",
    "print('MAE without x:', mean_absolute_error(y, zy_slope.predict(X)))\n",
    "\n",
    "# Will use this model later\n",
    "xzy_slope = LinearRegression()\n",
    "X = points_df[['x', 'z']]\n",
    "y = points_df['y']\n",
    "xzy_slope.fit(X, y)\n",
    "print('MAE with x:', mean_absolute_error(y, xzy_slope.predict(X)))\n",
    "\n",
    "print('\\ndy/dx = {:.3f}\\ndy/dz = {:.3f}'.format(*xzy_slope.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3d Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from math import sin, cos\n",
    "\n",
    "# convert euler angle to rotation matrix\n",
    "def euler_to_Rot(yaw, pitch, roll):\n",
    "    Y = np.array([[cos(yaw), 0, sin(yaw)],\n",
    "                  [0, 1, 0],\n",
    "                  [-sin(yaw), 0, cos(yaw)]])\n",
    "    P = np.array([[1, 0, 0],\n",
    "                  [0, cos(pitch), -sin(pitch)],\n",
    "                  [0, sin(pitch), cos(pitch)]])\n",
    "    R = np.array([[cos(roll), -sin(roll), 0],\n",
    "                  [sin(roll), cos(roll), 0],\n",
    "                  [0, 0, 1]])\n",
    "    return np.dot(Y, np.dot(P, R))\n",
    "\n",
    "def draw_line(image, points):\n",
    "    color = (255, 0, 0)\n",
    "    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n",
    "    return image\n",
    "\n",
    "def draw_points(image, points):\n",
    "    for (p_x, p_y, p_z) in points:\n",
    "#         cv2.circle(image, (p_x, p_y), int(1000 / p_z), (0, 255, 0), -1)\n",
    "        cv2.circle(image, (p_x, p_y), int(30), (0, 255, 0), -1)\n",
    "#         if p_x > image.shape[1] or p_y > image.shape[0]:\n",
    "#             print('Point', p_x, p_y, 'is out of image with shape', image.shape)\n",
    "    return image\n",
    "\n",
    "def imread(file_name,fast_mode=False):\n",
    "    img = cv2.imread(file_name)\n",
    "    if not fast_mode and img is not None and len(img.shape) == 3:\n",
    "        img = np.array(img[:, :, ::-1])\n",
    "    return img\n",
    "\n",
    "def visualize(img, coords):\n",
    "    # You will also need functions from the previous cells\n",
    "    x_l = 1.02\n",
    "    y_l = 0.80\n",
    "    z_l = 2.31\n",
    "    \n",
    "    img = img.copy()\n",
    "    for point in coords:\n",
    "        # Get values\n",
    "        x, y, z = point['x'], point['y'], point['z']\n",
    "        yaw, pitch, roll = -point['pitch'], -point['yaw'], -point['roll']\n",
    "        # Math\n",
    "        Rt = np.eye(4)\n",
    "        t = np.array([x, y, z])\n",
    "        \n",
    "#         print(Rt)\n",
    "#         print(t)\n",
    "        \n",
    "        Rt[:3, 3] = t\n",
    "        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n",
    "        Rt = Rt[:3, :]\n",
    "\n",
    "#         print(Rt)\n",
    "        P = np.array([[x_l, -y_l, -z_l, 1],\n",
    "                      [x_l, -y_l, z_l, 1],\n",
    "                      [-x_l, -y_l, z_l, 1],\n",
    "                      [-x_l, -y_l, -z_l, 1],\n",
    "                      [0, 0, 0, 1]]).T\n",
    "        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n",
    "        img_cor_points = img_cor_points.T\n",
    "        img_cor_points[:, 0] /= img_cor_points[:, 2]\n",
    "        img_cor_points[:, 1] /= img_cor_points[:, 2]\n",
    "        img_cor_points = img_cor_points.astype(int)\n",
    "        # Drawing\n",
    "        img = draw_line(img, img_cor_points)\n",
    "        img = draw_points(img, img_cor_points[-1:])\n",
    "    return img\n",
    "\n",
    "# train_root = \"./dataset/train_images/\"\n",
    "# for idx,_ in enumerate(os.listdir(train_root)):\n",
    "#     if idx<1:\n",
    "#         name = train['ImageId'][idx]\n",
    "#         img = cv_read(\"{}/{}.jpg\".format(train_root,name))\n",
    "#         coordinate = str2coords(train['PredictionString'].iloc[idx])\n",
    "#         pose_info = visualize(img,coordinate)\n",
    "\n",
    "#         print(np.shape(img))\n",
    "#         fig,axes = plt.subplots(1,2,figsize=(12,12))\n",
    "#         axes[0].imshow(img)\n",
    "#         img_vis = visualize(img, str2coords(train['PredictionString'].iloc[idx]))\n",
    "#         axes[1].imshow(img_vis)\n",
    "#         plt.pause(.1)\n",
    "#     else:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_WIDTH = 1024\n",
    "# IMG_HEIGHT = 320  #IMG_HEIGHT = IMG_WIDTH // 16 * 5\n",
    "\n",
    "# IMG_WIDTH = 1536\n",
    "# IMG_HEIGHT = 512\n",
    "# MODEL_SCALE = 8\n",
    "\n",
    "# IMG_WIDTH = 2048\n",
    "# IMG_HEIGHT = IMG_WIDTH // 4\n",
    "\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "MODEL_SCALE = 8\n",
    "\n",
    "\n",
    "def imread(path, fast_mode=False):\n",
    "    img = cv2.imread(path)\n",
    "    if not fast_mode and img is not None and len(img.shape) == 3:\n",
    "        img = np.array(img[:, :, ::-1])\n",
    "    return img\n",
    "\n",
    "def rotate(x, angle): \n",
    "    x = x + angle \n",
    "    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi \n",
    "    return x\n",
    "\n",
    "def _regr_preprocess(regr_dict, flip=False):\n",
    "    if flip:\n",
    "        for k in ['x', 'pitch', 'roll']:\n",
    "            regr_dict[k] = -regr_dict[k]\n",
    "    for name in ['x', 'y', 'z']:\n",
    "        regr_dict[name] = regr_dict[name] / 100\n",
    "    regr_dict['roll'] = rotate(regr_dict['roll'], np.pi)\n",
    "    regr_dict['pitch_sin'] = sin(regr_dict['pitch'])\n",
    "    regr_dict['pitch_cos'] = cos(regr_dict['pitch'])\n",
    "    regr_dict.pop('pitch')\n",
    "    regr_dict.pop('id')\n",
    "    return regr_dict\n",
    "\n",
    "def _regr_back(regr_dict):\n",
    "    for name in ['x', 'y', 'z']:\n",
    "        regr_dict[name] = regr_dict[name] * 100\n",
    "    regr_dict['roll'] = rotate(regr_dict['roll'], -np.pi)\n",
    "    \n",
    "    pitch_sin = regr_dict['pitch_sin'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
    "    pitch_cos = regr_dict['pitch_cos'] / np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n",
    "    regr_dict['pitch'] = np.arccos(pitch_cos) * np.sign(pitch_sin)\n",
    "    return regr_dict\n",
    "\n",
    "def preprocess_image(img, flip=False):\n",
    "    img = img[img.shape[0] // 2:]\n",
    "#     bg = np.ones_like(img) * img.mean(1, keepdims=True).astype(img.dtype)\n",
    "#     bg = bg[:, :img.shape[1] // 6]\n",
    "#     img = np.concatenate([bg, img, bg], 1)\n",
    "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    if flip:\n",
    "        img = img[:,::-1]\n",
    "    return (img / 255).astype('float32')\n",
    "\n",
    "def get_mask_and_regr(img, labels, flip=False):\n",
    "    mask = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE], dtype='float32')\n",
    "    regr_names = ['x', 'y', 'z', 'yaw', 'pitch', 'roll']\n",
    "    regr = np.zeros([IMG_HEIGHT // MODEL_SCALE, IMG_WIDTH // MODEL_SCALE, 7], dtype='float32')\n",
    "    coords = str2coords(labels)\n",
    "    xs, ys = get_img_coords(labels)\n",
    "    for x, y, regr_dict in zip(xs, ys, coords):\n",
    "        x, y = y, x\n",
    "        x = (x - img.shape[0] // 2) * IMG_HEIGHT / (img.shape[0] // 2) / MODEL_SCALE\n",
    "        x = np.round(x).astype('int')\n",
    "#         y = (y + img.shape[1] // 6) * IMG_WIDTH / (img.shape[1] * 4/3) / MODEL_SCALE\n",
    "        y = (y) * IMG_WIDTH / (img.shape[1]) / MODEL_SCALE\n",
    "        y = np.round(y).astype('int')\n",
    "        if x >= 0 and x < IMG_HEIGHT // MODEL_SCALE and y >= 0 and y < IMG_WIDTH // MODEL_SCALE:\n",
    "            mask[x, y] = 1\n",
    "            regr_dict = _regr_preprocess(regr_dict, flip)\n",
    "            regr[x, y] = [regr_dict[n] for n in sorted(regr_dict)]\n",
    "    if flip:\n",
    "        mask = np.array(mask[:,::-1])\n",
    "        regr = np.array(regr[:,::-1])\n",
    "    return mask, regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './dataset/'\n",
    "# img = imread(PATH + 'train_images/' + train_pd['ImageId'][0] + '.jpg')\n",
    "img = imread(PATH + 'masked_train/' + train_pd['ImageId'][20] + '.jpg')\n",
    "print(train_pd['ImageId'][20])\n",
    "img_pre = preprocess_image(img)\n",
    "mask, regr = get_mask_and_regr(img, train_pd['PredictionString'][20])\n",
    "\n",
    "img_f = preprocess_image(img,flip=True)\n",
    "mask_f, regr_f = get_mask_and_regr(img, train_pd['PredictionString'][20],flip=True)\n",
    "\n",
    "print('img.shape', img.shape, 'std:', np.std(img))\n",
    "print('pre img.shape', img_pre.shape, 'std:', np.std(img_pre))\n",
    "print('mask.shape', mask.shape, 'std:', np.std(mask))\n",
    "print('regr.shape', regr.shape, 'std:', np.std(regr))\n",
    "\n",
    "# fig, axes = plt.subplots(3,2,figsize=(16,16))\n",
    "# axes[0][0].imshow(img_pre)\n",
    "# axes[0][1].imshow(img_f)\n",
    "\n",
    "# import scipy.ndimage\n",
    "# mask_up =  scipy.ndimage.zoom(mask, 8, order=1)\n",
    "# axes[1][0].imshow(mask_up)\n",
    "# axes[1][0].imshow(img_pre,alpha=0.4)\n",
    "\n",
    "# mask_up_f =  scipy.ndimage.zoom(mask_f, 8, order=1)\n",
    "# axes[1][1].imshow(mask_up_f)\n",
    "# axes[1][1].imshow(img_f,alpha=0.4)\n",
    "\n",
    "# axes[2][0].imshow(regr[:,:,-2])\n",
    "# axes[2][1].imshow(regr_f[:,:,-2])\n",
    "\n",
    "# for idx in range(2):\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
    "    \n",
    "#     for ax_i in range(2):\n",
    "#         img0 = imread(PATH + 'train_images/' + train['ImageId'].iloc[idx] + '.jpg')\n",
    "#         if ax_i == 1:\n",
    "#             img0 = img0[:,::-1]\n",
    "#         img = preprocess_image(img0, ax_i==1)\n",
    "#         mask, regr = get_mask_and_regr(img0, train['PredictionString'][idx], ax_i==1)\n",
    "#         regr = np.rollaxis(regr, 2, 0)\n",
    "#         coords = extract_coords(np.concatenate([mask[None], regr], 0), ax_i==1)\n",
    "        \n",
    "#         axes[ax_i].set_title('Flip = {}'.format(ax_i==1))\n",
    "#         axes[ax_i].imshow(visualize(img0, coords))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(size,size))\n",
    "# plt.title('Processed image')W\n",
    "# plt.imshow(img_pre)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(size,size))\n",
    "# plt.title('Detection Mask')\n",
    "# plt.imshow(mask)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(size,size))\n",
    "# plt.title('Yaw values')\n",
    "# plt.imshow(regr[:,:,-2])\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(0,1):\n",
    "#     regr = np.transpose(regr,(2,0,1))\n",
    "#     coords_true = extract_coords(np.concatenate([mask[None], regr], 0))\n",
    "#     fig, axes = plt.subplots(1,1, figsize=(8,8))\n",
    "#     axes.set_title('Ground truth')\n",
    "#     axes.imshow(visualize(img, coords_true))\n",
    "#     plt.pause(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "# from imgaug.augmentables.segmaps import SegmentationMapOnImage\n",
    "class ImgAugTransform:\n",
    "    def __init__(self):\n",
    "        self.aug = iaa.Sequential([\n",
    "#         iaa.Scale((640, 480)),\n",
    "#         iaa.Fliplr(0.5),\n",
    "#         iaa.Sometimes(0.3, iaa.GaussianBlur(sigma=(0, 0.75))),\n",
    "#         iaa.Sometimes(0.1, iaa.AverageBlur(1.2)),\n",
    "#         iaa.Sometimes(1, iaa.Affine(rotate=(-20, 20),order=[0, 1],translate_px={\"x\":(-2, 2),\"y\":(-2,2)},mode='symmetric')),\n",
    "#         iaa.Sometimes(0.2,iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.25))),\n",
    "#         iaa.Sometimes(0.1, iaa.SaltAndPepper(0.05,False)),\n",
    "#         iaa.Invert(0.5),\n",
    "#         iaa.Add((-5, 5)), # change brightness of images (by -10 to 10 of original value)\n",
    "        iaa.Sometimes(0.5, iaa.AdditiveGaussianNoise(0,0.01*255)),\n",
    "        iaa.Sometimes(0.5,iaa.GammaContrast((0.3,1.5))),\n",
    "#         iaa.AddToHueAndSaturation(from_colorspace=\"RGB\",value=(-20, 20))  #Hue-> color, saturation -> saido\n",
    "    ])\n",
    "    def __call__(self, img, mask=None):\n",
    "        img = np.array(img)        \n",
    "        return self.aug.augment_image(image=img)\n",
    "#         return self.aug(image=img, segmentation_maps=label)\n",
    "\n",
    "\n",
    "\n",
    "# train distribution: mean=[0.2732385  0.28948802 0.31470126],std=[0.19721317 0.20766443 0.20611505]\n",
    "# test distribution: mean=[0.26370072 0.28066522 0.30648127],std=[0.19954063 0.20964707 0.2084653 ]\n",
    "trans = transforms.Compose([\n",
    "#         transforms.ColorJitter(0.,0.2,0.,0.),\n",
    "#         transforms.RandomAffine(degrees=20,translate=(0.25,0.25),scale=[0.65,1.1],shear=15), #after 60k\n",
    "#         transforms.RandomAffine(degrees=15,translate=(0.25,0.25),scale=[0.7,1.1],shear=8), #60k baseline\n",
    "        ImgAugTransform(),\n",
    "        lambda x: Image.fromarray(x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.2732385,0.28948802,0.31470126],std=[0.19721317,0.20766443,0.20611505])\n",
    "    ])\n",
    "\n",
    "trans_val = transforms.Compose([\n",
    "        transforms.ToTensor(),  #Take Image as input and convert to tensor with value from 0 to1\n",
    "        transforms.Normalize(mean=[0.2732385,0.28948802,0.31470126],std=[0.19721317,0.20766443,0.20611505])\n",
    "    ])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.26370072,0.28066522,0.30648127],std=[0.19954063,0.20964707,0.2084653])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv(\"./dataset/train_remove.csv\")\n",
    "# train_pd = pd.read_csv(\"./dataset/train.csv\")\n",
    "test_pd = pd.read_csv(\"./dataset/sample_submission.csv\")\n",
    "\n",
    "class ADDataset(Dataset):\n",
    "    def __init__(self,data_len=None, is_validate=False,validate_rate=None,indices=None):\n",
    "        self.is_validate = is_validate\n",
    "#         self.data = global_car_data\n",
    "        if data_len == None:\n",
    "            data_len = len(self.data)\n",
    "        \n",
    "        self.indices = indices\n",
    "        if self.is_validate:\n",
    "            self.len = int(np.ceil(data_len*validate_rate))\n",
    "            self.offset = int(data_len*(1-validate_rate))\n",
    "            self.transform = trans_val\n",
    "        else:\n",
    "            self.len = int(data_len*(1-validate_rate))\n",
    "            self.offset = 0\n",
    "            self.transform = trans\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        idx += self.offset\n",
    "        idx = self.indices[idx]\n",
    "        img = cv2.imread('./dataset/masked_train/' + train_pd['ImageId'].iloc[idx] + '.jpg')\n",
    "#         img = cv2.imread('./dataset/train_images/' + train_pd['ImageId'].iloc[idx] + '.jpg')\n",
    "    \n",
    "        img = np.array(img[:,:,::-1])\n",
    "\n",
    "        mask, regr = get_mask_and_regr(img, train_pd['PredictionString'][idx])\n",
    "        img_pre = preprocess_image(img)  #shape(batch,512,512), #range: [0~1]\n",
    "        \n",
    "        img_pil = (img_pre*255).astype('uint8')\n",
    "        img_pil = Image.fromarray(img_pil)  #ndarray: Take uint8 as input, range[0~255], #imgpil -> (512,512,3), (0~255)\n",
    "        img_pre_trans = self.transform(img_pil)\n",
    "        \n",
    "#         print(np.shape(img))        #(2710, 3384, 3)\n",
    "#         print(np.shape(img_pre))    #(320, 1024, 3)\n",
    "#         print(np.shape(mask))       #(40, 128)\n",
    "#         print(np.shape(regr))       #(40, 128,7)\n",
    "        \n",
    "        regr = np.transpose(regr,(2,0,1))\n",
    "        \n",
    "#         label = torch.as_tensor(label, dtype=torch.uint8)    #value: 0~9, shape(1)\n",
    "#         img_pre = torch.as_tensor(img_pre, dtype=torch.float32) #For distribution computing\n",
    "\n",
    "        return img_pre_trans, mask, regr\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,data_len=None):\n",
    "        self.transform = trans_test\n",
    "        self.len = data_len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv_read('./dataset/masked_test/{}.jpg'.format(test_pd['ImageId'].iloc[idx]))\n",
    "#         img = cv_read('./dataset/test/{}.jpg'.format(test_pd['ImageId'].iloc[idx]))\n",
    "        \n",
    "        img = np.array(img[:,:,::-1])\n",
    "        img_pre = preprocess_image(img)  #shape(batch,512,512), #range: [0~1]\n",
    "        img_pil = (img_pre*255).astype('uint8')\n",
    "        img_pil = Image.fromarray(img_pil)  #ndarray: Take uint8 as input, range[0~255], #imgpil -> (512,512,3), (0~255)\n",
    "        img_pre_trans = self.transform(img_pil)        \n",
    "\n",
    "        return img_pre_trans   #return img_pre_trans, _ will cause strange behavior (load very slow after 10 batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Ditribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train distribution: mean=[0.08229437],std=[0.23876116]\n",
    "# dig augmented distribution: mean=[0.09549136],std=[0.24336776]\n",
    "# train large distribution: mean=[0.08889286],std=[0.24106438]\n",
    "\n",
    "def get_dataset_mean_std(dataloader):\n",
    "    print(\"Calculate distribution:\")\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    for data in dataloader:\n",
    "        img = data[0].to(device)\n",
    "        \n",
    "#         print(np.shape(img)) #(batch,3,512,512)\n",
    "        \n",
    "        batch_samples = img.size(0)\n",
    "        img = img.contiguous().view(batch_samples, img.size(1), -1)\n",
    "        mean += img.mean(2).sum(0)\n",
    "        std += img.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "        if nb_samples%640 == 0:\n",
    "            print(\"Finished:\", nb_samples)\n",
    "            \n",
    "    print(\"num of samples:\",nb_samples)\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "#     print(\"Average mean:\",mean)\n",
    "#     print(\"Average std:\", std)\n",
    "    return mean.cpu().numpy(), std.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # for padding issues, see \n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        \n",
    "        if x2 is not None:\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "def get_mesh(batch_size, shape_x, shape_y):\n",
    "    mg_x, mg_y = np.meshgrid(np.linspace(0, 1, shape_y), np.linspace(0, 1, shape_x))\n",
    "    mg_x = np.tile(mg_x[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
    "    mg_y = np.tile(mg_y[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n",
    "    mesh = torch.cat([torch.tensor(mg_x).to(device), torch.tensor(mg_y).to(device)], 1)\n",
    "    return mesh\n",
    "\n",
    "class MyUNet(nn.Module):\n",
    "    '''Mixture of previous classes'''\n",
    "    def __init__(self, n_classes):\n",
    "        super(MyUNet, self).__init__()\n",
    "        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "#         self.base_model = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "#         self.base_model = EfficientNet.from_pretrained('efficientnet-b5')\n",
    "#         self.base_model = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        \n",
    "        self.conv0 = double_conv(5, 128)\n",
    "        self.conv1 = double_conv(128, 256)\n",
    "        self.conv2 = double_conv(256, 512)\n",
    "        self.conv3 = double_conv(512, 1024)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.dp = nn.Dropout(0.5)\n",
    "        \n",
    "        ###eff-b0\n",
    "        self.up1 = up(1282 + 1024, 512)\n",
    "        self.up2 = up(512 + 512, 256)\n",
    "\n",
    "        ###eff-b4\n",
    "#         self.up1 = up(1794 + 1024, 512)\n",
    "#         self.up2 = up(512 + 512, 256)\n",
    "        \n",
    "        ###eff-b5\n",
    "#         self.up1 = up(2050 + 1024, 512)\n",
    "#         self.up2 = up(512 + 512, 256)        \n",
    "        \n",
    "        ###eff-b7\n",
    "#         self.up1 = up(2562 + 1024, 512)\n",
    "#         self.up2 = up(512 + 512, 256)\n",
    "        \n",
    "        \n",
    "        self.outc = nn.Conv2d(256, n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        mesh1 = get_mesh(batch_size, x.shape[2], x.shape[3])\n",
    "        x0 = torch.cat([x, mesh1], 1)\n",
    "        x1 = self.dp(self.mp(self.conv0(x0)))\n",
    "        x2 = self.dp(self.mp(self.conv1(x1)))\n",
    "        x3 = self.dp(self.mp(self.conv2(x2)))\n",
    "        x4 = self.dp(self.mp(self.conv3(x3)))\n",
    "        \n",
    "#         x_center = x[:, :, :, IMG_WIDTH // 8: -IMG_WIDTH // 8]\n",
    "        x_center = x[:, :, :, :]\n",
    "        \n",
    "        feats = self.base_model.extract_features(x_center)    #[1, 1280, 16, 24] for eff-b0\n",
    "#         print(\"eff extract size:\",feats.size())\n",
    "        \n",
    "        ### Pad with zero (don't know why)\n",
    "#         bg = torch.zeros([feats.shape[0], feats.shape[1], feats.shape[2], feats.shape[3] // 8]).to(device)\n",
    "#         print(\"bg size:\",bg.size())\n",
    "#         feats = torch.cat([bg, feats, bg], 3)\n",
    "#         print(\"feats size:\",feats.size())\n",
    "        \n",
    "        # Add positional info\n",
    "        mesh2 = get_mesh(batch_size, feats.shape[2], feats.shape[3])\n",
    "#         print(\"mesh2 size:\",mesh2.size())\n",
    "        feats = torch.cat([feats, mesh2], 1)\n",
    "        \n",
    "#         print(\"feats size:\",feats.size())\n",
    "#         print(\"x4 size:\", x4.size())\n",
    "#         print(\"x3 size:\", x3.size())\n",
    "\n",
    "        ###eff-b0\n",
    "#         feats: torch.Size([1, 1282, 16, 44])\n",
    "#         x4: torch.Size([1, 1024, 32, 96])\n",
    "#         x3: torch.Size([1, 512, 64, 192])\n",
    "\n",
    "        ###eff-b7\n",
    "#         feats: torch.Size([1, 2562, 16, 44])\n",
    "#         x4: torch.Size([1, 1024, 32, 96])\n",
    "#         x3: torch.Size([1, 512, 64, 192])\n",
    "        \n",
    "        x = self.up1(feats, x4)\n",
    "#         print(\"up1 size:\",x.size())\n",
    "        x = self.up2(x, x3)\n",
    "#         print(\"up2 size:\",x.size())\n",
    "        x = self.outc(x)\n",
    "        return x\n",
    "    \n",
    "#     eff extract size: torch.Size([1, 1280, 16, 24])\n",
    "#     bg size: torch.Size([1, 1280, 16, 3])\n",
    "#     feats size: torch.Size([1, 1280, 16, 30])\n",
    "#     mesh2 size: torch.Size([1, 2, 16, 30])\n",
    "#     feats size: torch.Size([1, 1282, 16, 30])\n",
    "#     x4 size: torch.Size([1, 1024, 32, 64])\n",
    "#     x3 size: torch.Size([1, 512, 64, 128])\n",
    "#     up1 size: torch.Size([1, 512, 32, 64])\n",
    "#     up2 size: torch.Size([1, 256, 64, 128])\n",
    "#     torch.Size([1, 8, 64, 128])        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CenterNet.src.lib.models.losses import FocalLoss, RegL1Loss, RegLoss, RegWeightedL1Loss, RegLoss_without_ind\n",
    "f_loss = FocalLoss()\n",
    "l1_loss = RegLoss_without_ind()\n",
    "\n",
    "def criterion_new(prediction, mask, regr, weight=0.5, result_average=True, multiloss=False):\n",
    "#     pred_mask = torch.sigmoid(prediction[:, 0])\n",
    "    pred_mask = torch.clamp(torch.sigmoid(prediction[:, 0]), min=1e-4, max=1-1e-4)\n",
    "    mask_loss = f_loss(pred_mask, mask)\n",
    "\n",
    "    ### task A 训练的收敛后，再把 A 和 B join 到一起训练\n",
    "    if multiloss:\n",
    "        pred_regr = prediction[:, 1:]\n",
    "        regr_loss = l1_loss(pred_regr,regr,mask)        \n",
    "        loss = mask_loss + regr_loss\n",
    "    else:\n",
    "        regr_loss = mask_loss\n",
    "        loss = mask_loss\n",
    "    \n",
    "    if not result_average:\n",
    "        loss *= prediction.shape[0]\n",
    "    return loss ,mask_loss , regr_loss\n",
    "\n",
    "\n",
    "def criterion(prediction, mask, regr,weight=0.4, result_average=True):\n",
    "    # Binary mask loss\n",
    "    pred_mask = torch.sigmoid(prediction[:, 0])\n",
    "#     mask_loss = mask * (1 - pred_mask)**2 * torch.log(pred_mask + 1e-12) + (1 - mask) * pred_mask**2 * torch.log(1 - pred_mask + 1e-12)\n",
    "    mask_loss = mask * torch.log(pred_mask + 1e-12) + (1 - mask) * torch.log(1 - pred_mask + 1e-12)\n",
    "    mask_loss = -mask_loss.mean(0).sum()\n",
    "    \n",
    "    # Regression L1 loss\n",
    "    pred_regr = prediction[:, 1:]\n",
    "    regr_loss = (torch.abs(pred_regr - regr).sum(1) * mask).sum(1).sum(1) / mask.sum(1).sum(1)\n",
    "    regr_loss = regr_loss.mean(0)\n",
    "  \n",
    "    # Sum\n",
    "    loss = weight*mask_loss +(1-weight)* regr_loss\n",
    "    if not result_average:\n",
    "        loss *= prediction.shape[0]\n",
    "    return loss ,mask_loss , regr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train indices: 4257\n"
     ]
    }
   ],
   "source": [
    "###Single dataset\n",
    "vr = 0.1\n",
    "batch_size = 3\n",
    "num_workers = 12\n",
    "\n",
    "train_pd = pd.read_csv(\"./dataset/train_remove.csv\")\n",
    "indices_len = len(train_pd)\n",
    "print(\"len of train indices:\",indices_len)\n",
    "indices = np.arange(indices_len)\n",
    "train_dataset = ADDataset(data_len=indices_len,is_validate=False,validate_rate=vr,indices=indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# mean, std = get_dataset_mean_std(train_loader)\n",
    "# print(\"train distribution: mean={},std={}\".format(mean, std))\n",
    "# train distribution: mean=[0.2732385  0.28948802 0.31470126],std=[0.19721317 0.20766443 0.20611505]\n",
    "\n",
    "val_dataset = ADDataset(data_len=indices_len,is_validate=True,validate_rate=vr,indices=indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# test_pd = pd.read_csv(\"./dataset/sample_submission.csv\")\n",
    "# indices_len = len(test_pd)\n",
    "# print(\"len of test indices:\",indices_len)\n",
    "# test_dataset = TestDataset(data_len=indices_len)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "# mean, std = get_dataset_mean_std(test_loader)\n",
    "# print(\"test distribution: mean={},std={}\".format(mean, std))\n",
    "# test distribution: mean=[0.26370072 0.28066522 0.30648127],std=[0.19954063 0.20964707 0.2084653 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightweight segnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from light_seg_model import get_segmentation_model\n",
    "# model = get_segmentation_model(\"efficientnet\", dataset=train_dataset, \n",
    "#                                aux=False, norm_layer=torch.nn.BatchNorm2d).to(device)\n",
    "# model.cuda()\n",
    "# print(\"Create model complete\")\n",
    "\n",
    "# # img, mask, regr = train_dataset[0]\n",
    "# # img = img[None]  #(1,3,512,512),  img[None] = img[np.newaxis]\n",
    "# # with torch.no_grad():\n",
    "# #     output = model(img.to(device))  #(1,8,64,192)\n",
    "# # print(output.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2ebd7cf13cba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model = CentResnet(8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# model.load_state_dict(torch.load(\"./saved_model/Ep5_loss8.3501\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# summary(model, input_size=(3,512,2048))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "# model = CentResnet(8)\n",
    "model = MyUNet(8)\n",
    "model.cuda()\n",
    "# model.load_state_dict(torch.load(\"./saved_model/Ep5_loss8.3501\"))\n",
    "# summary(model, input_size=(3,512,2048))\n",
    "\n",
    "img, mask, regr = train_dataset[0]\n",
    "size = 8\n",
    "img = img[None]  #(1,3,512,512),  img[None] = img[np.newaxis]\n",
    "with torch.no_grad():\n",
    "    output = model(img.to(device))  #(1,8,64,192)\n",
    "    logits = output.cpu().numpy()\n",
    "print(output.size())\n",
    "\n",
    "# print(np.max(logits))\n",
    "# plt.figure(figsize=(size,size))\n",
    "# plt.title('Model predictions')\n",
    "# plt.imshow(logits)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = None\n",
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "# torch.cuda.empty_cache()\n",
    "# from AdaBound import adabound\n",
    "\n",
    "lr = 2.5e-4\n",
    "lr_period = 5\n",
    "epochs = 100\n",
    "val_freq = 1\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=lr,betas=(0.9,0.99))\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# optimizer = adabound.AdaBound(model.parameters(), lr=lr, final_lr=1e-2)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, alpha=0.9,)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=5,factor=0.1)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(epochs, 10) * len(train_loader) // 3, gamma=0.1)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=lr_period,T_mult=1,eta_min=1e-6) #original \n",
    "\n",
    "min_loss = 100000\n",
    "prev_val_loss = 1000\n",
    "loss_th = 2\n",
    "multiloss = True\n",
    "best_model_dict = None\n",
    "\n",
    "import gc\n",
    "history = pd.DataFrame()\n",
    "\n",
    "for ep in range(1,epochs+1):\n",
    "    model.train()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "    for batch_idx, (img_batch, mask_batch, regr_batch) in enumerate(tqdm(train_loader)):\n",
    "        img_batch = img_batch.to(device)\n",
    "        mask_batch = mask_batch.to(device)\n",
    "        regr_batch = regr_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#         print(img_batch.size())  #(batch,3,512,2048)\n",
    "#         print(mask_batch.size()) #(batch,64,256)\n",
    "#         print(regr_batch.size()) #(batch,7,64,256)\n",
    "        output = model(img_batch)\n",
    "#         print(output.size())    #(batch,8,64,256)\n",
    "\n",
    "        loss, mask_loss , regr_loss = criterion_new(output, mask_batch, regr_batch, multiloss=multiloss)\n",
    "#         if loss.item()<=loss_th:\n",
    "#             multiloss = True\n",
    "#         else:\n",
    "#             multiloss = False\n",
    "#         print(loss.item())\n",
    "#         print(mask_loss.item())\n",
    "#         print(regr_loss.item())\n",
    "    \n",
    "        if history is not None:\n",
    "            history.loc[ep + batch_idx / len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch:{} lr:{:.5f} loss:{:.6f} mloss:{:.6f} rloss{:.6f}'.\n",
    "          format(ep,optimizer.param_groups[0]['lr'],loss.data,mask_loss.data,regr_loss.data))\n",
    "#     lr_scheduler.step()\n",
    "\n",
    "    if ep%val_freq==0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        data_num = 0\n",
    "        with torch.no_grad():\n",
    "            for img_batch, mask_batch, regr_batch in val_loader:\n",
    "                img_batch = img_batch.to(device)\n",
    "                mask_batch = mask_batch.to(device)\n",
    "                regr_batch = regr_batch.to(device)\n",
    "                output = model(img_batch)\n",
    "#                 val_loss += criterion_new(output, mask_batch, regr_batch, result_average=False)[0].data\n",
    "                val_loss += criterion_new(output, mask_batch, regr_batch, result_average=False,multiloss=True)[0].data\n",
    "                img_batch.size()\n",
    "                data_num += img_batch.size(0)\n",
    "                \n",
    "        val_loss /= data_num\n",
    "        print('Val loss: {:.4f}'.format(val_loss))\n",
    "        prev_val_loss = val_loss\n",
    "        \n",
    "        lr_scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            best_model_dict = model.state_dict()\n",
    "            print(\"./saved_model/Ep:{}_loss{:.4f}\".format(ep,min_loss))\n",
    "            torch.save(best_model_dict, \"./saved_model/Ep{}_loss{:.4f}\".format(ep,min_loss))\n",
    "        \n",
    "#         if history is not None:\n",
    "#             history.loc[epoch, 'dev_loss'] = loss.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history['train_loss'].iloc[100:].plot()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-72cdd5556290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# img, _ = test_dataset[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./saved_model/Ep6_loss0.6735\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch2/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "mean,std = [0.2732385,0.28948802,0.31470126],[0.19721317,0.20766443,0.20611505] #train \n",
    "# mean,std =[0.26370072,0.28066522,0.30648127],[0.19954063,0.20964707,0.2084653 ] #test\n",
    "mean,std = np.array(mean), np.array(std)\n",
    "\n",
    "def denormalize(img):\n",
    "    img = transforms.Normalize(-1*mean/std, 1/std)(img) #denormalize\n",
    "    return img\n",
    "\n",
    "img, mask, regr = train_dataset[1]\n",
    "# img, mask, regr = val_dataset[0]\n",
    "# img, _ = test_dataset[1]\n",
    "\n",
    "model.load_state_dict(torch.load(\"./saved_model/Ep6_loss0.6735\"))\n",
    "model.cuda()\n",
    "\n",
    "size = 6\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.title('Input image')\n",
    "\n",
    "img_show = denormalize(img).permute(1,2,0).cpu().numpy()\n",
    "up = np.max(img_show)\n",
    "low = np.min(img_show)\n",
    "print(up,low)\n",
    "plt.imshow(img_show)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.title('Ground truth mask')\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "img = img[None]  #(1,3,512,512),  img[None] = img[np.newaxis]\n",
    "print(np.shape(img))\n",
    "with torch.no_grad():\n",
    "    output = model(img.to(device))  #(1,8,64,256)\n",
    "    logits = output[0][0].data.cpu().numpy()      \n",
    "\n",
    "print(output.size())\n",
    "print(np.max(logits))\n",
    "\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.title('Model predictions')\n",
    "plt.imshow(logits)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(size,size))\n",
    "plt.title('Model predictions thresholded')\n",
    "plt.imshow(logits > 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d to 3d inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread(\"./dataset/train_images/{}.jpg\".format(train_pd['ImageId'].iloc[0]))\n",
    "IMG_SHAPE = img.shape\n",
    "DISTANCE_THRESH_CLEAR = 2\n",
    "\n",
    "def convert_3d_to_2d(x, y, z, fx = 2304.5479, fy = 2305.8757, cx = 1686.2379, cy = 1354.9849):\n",
    "    return x * fx / z + cx, y * fy / z + cy\n",
    "\n",
    "def optimize_xy(r, c, x0, y0, z0, flipped=False):\n",
    "    def distance_fn(xyz):\n",
    "        x, y, z = xyz\n",
    "        xx = -x if flipped else x\n",
    "        slope_err = (xzy_slope.predict([[xx,z]])[0] - y)**2\n",
    "        x, y = convert_3d_to_2d(x, y, z)\n",
    "        y, x = x, y\n",
    "        x = (x - IMG_SHAPE[0] // 2) * IMG_HEIGHT / (IMG_SHAPE[0] // 2) / MODEL_SCALE\n",
    "#         y = (y + IMG_SHAPE[1] // 6) * IMG_WIDTH / (IMG_SHAPE[1] * 4 / 3) / MODEL_SCALE\n",
    "        y = (y) * IMG_WIDTH / (IMG_SHAPE[1]) / MODEL_SCALE\n",
    "        \n",
    "        return max(0.2, (x-r)**2 + (y-c)**2) + max(0.4, slope_err)\n",
    "    \n",
    "    res = minimize(distance_fn, [x0, y0, z0], method='Powell')\n",
    "    x_new, y_new, z_new = res.x\n",
    "    return x_new, y_new, z_new\n",
    "\n",
    "def clear_duplicates(coords):\n",
    "    for c1 in coords:\n",
    "        xyz1 = np.array([c1['x'], c1['y'], c1['z']])\n",
    "        for c2 in coords:\n",
    "            xyz2 = np.array([c2['x'], c2['y'], c2['z']])\n",
    "            distance = np.sqrt(((xyz1 - xyz2)**2).sum())\n",
    "            if distance < DISTANCE_THRESH_CLEAR:\n",
    "                if c1['confidence'] < c2['confidence']:\n",
    "                    c1['confidence'] = -1\n",
    "    return [c for c in coords if c['confidence'] > 0]\n",
    "\n",
    "def extract_coords(prediction, flipped=False):\n",
    "    logits = prediction[0]\n",
    "    regr_output = prediction[1:]\n",
    "    points = np.argwhere(logits > 0)\n",
    "    col_names = sorted(['x', 'y', 'z', 'yaw', 'pitch_sin', 'pitch_cos', 'roll'])\n",
    "    coords = []\n",
    "    for r, c in points:\n",
    "        regr_dict = dict(zip(col_names, regr_output[:, r, c]))\n",
    "        coords.append(_regr_back(regr_dict))\n",
    "        coords[-1]['confidence'] = 1 / (1 + np.exp(-logits[r, c]))\n",
    "        coords[-1]['x'], coords[-1]['y'], coords[-1]['z'] = \\\n",
    "                optimize_xy(r, c,\n",
    "                            coords[-1]['x'],\n",
    "                            coords[-1]['y'],\n",
    "                            coords[-1]['z'], flipped)\n",
    "    coords = clear_duplicates(coords)\n",
    "    return coords\n",
    "\n",
    "def coords2str(coords, names=['yaw', 'pitch', 'roll', 'x', 'y', 'z', 'confidence']):\n",
    "    s = []\n",
    "    for c in coords:\n",
    "        for n in names:\n",
    "            s.append(str(c.get(n, 0)))\n",
    "    return ' '.join(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on val dataset for calculating mAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ths = [-1,0]\n",
    "# model = MyUNet(8)\n",
    "# model.cuda()\n",
    "# model.load_state_dict(torch.load(\"./saved_model/Ep2_loss0.3743\"))\n",
    "\n",
    "# model.eval()\n",
    "# val_pd = pd.read_csv(\"./dataset/train_remove.csv\")\n",
    "# vr = 0.1\n",
    "# print(len(val_pd))\n",
    "# val_pd = val_pd[int(len(val_pd)*(1-vr)):]\n",
    "# print(len(val_pd))\n",
    "\n",
    "# for threshold in ths:\n",
    "#     predictions = []\n",
    "#     indices_len = len(val_pd)\n",
    "#     print(\"indices_len\",indices_len)\n",
    "# #     val_dataset = ADDataset(data_len=indices_len,is_validate=True,validate_rate=vr,indices=indices)\n",
    "# #     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "#     with torch.no_grad():\n",
    "#         for img,_,_ in tqdm(val_loader):\n",
    "#             img = img.to(device)\n",
    "#             output = model(img).cpu().numpy()\n",
    "#             for out in output:\n",
    "#                 coords = extract_coords(out,logits_thresh=threshold)\n",
    "#                 s = coords2str(coords)\n",
    "#                 predictions.append(s)\n",
    "\n",
    "#     print(np.shape(predictions))\n",
    "#     val_pd['PredictionString']= predictions\n",
    "#     file_name = \"val_pred_th{}.csv\".format(threshold)\n",
    "#     val_pd.to_csv(file_name, index=False)\n",
    "#     val_pd.head()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Inference result on real Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(train_pd)\n",
    "test_pd = pd.read_csv(\"./dataset/sample_submission.csv\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with torch.no_grad():\n",
    "        for i in range(105,115):\n",
    "            img, mask, regr = val_dataset[i]\n",
    "#             img, _ = test_dataset[i]\n",
    "            \n",
    "            img = img.to(device).unsqueeze(0)\n",
    "            output = model(img).cpu().numpy()\n",
    "#             coords_pred = extract_coords(output[0],logits_thresh=-1.25)\n",
    "            coords_pred = extract_coords(output[0],logits_thresh=0)\n",
    "            coords_true = extract_coords(np.concatenate([mask[None], regr], 0))\n",
    "\n",
    "            offset = int(data_len*(1-vr))\n",
    "            offset = 0\n",
    "            idx = i+offset\n",
    "            img_origin = imread(\"./dataset/train_images/{}.jpg\".format(train_pd['ImageId'].iloc[idx]))\n",
    "#             img_origin = imread(\"./dataset/test_images/{}.jpg\".format(test_pd['ImageId'].iloc[idx]))\n",
    "            print(test_pd['ImageId'].iloc[idx])\n",
    "\n",
    "            print(\"ground true shape\", np.shape(coords_true))\n",
    "            print(\"pred shape\", np.shape(coords_pred))\n",
    "        #     print(coords_true)\n",
    "        #     print(\"\")\n",
    "        #     print(coords_pred)\n",
    "            fig, axes = plt.subplots(1,2, figsize=(16,16))\n",
    "            axes[0].set_title('Ground truth')\n",
    "            axes[0].imshow(visualize(img_origin, coords_true))\n",
    "            axes[1].set_title('Prediction')\n",
    "            axes[1].imshow(visualize(img_origin, coords_pred))\n",
    "            plt.pause(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = [0,-1]\n",
    "# model = CentResnet(8)\n",
    "# model.cuda()\n",
    "# model.load_state_dict(torch.load(\"./saved_model/res50_vr0.05_imgaugv2/Ep7_loss8.9245\"))\n",
    "torch.cuda.empty_cache()\n",
    "model = MyUNet(8)\n",
    "model.cuda()\n",
    "model.load_state_dict(best_model_dict)\n",
    "model.eval()\n",
    "\n",
    "for threshold in ths:\n",
    "    predictions = []\n",
    "    test_pd = pd.read_csv(\"./dataset/sample_submission.csv\")\n",
    "    indices_len = len(test_pd)\n",
    "    print(\"indices_len\",indices_len)\n",
    "    test_dataset = TestDataset(data_len=indices_len)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(test_loader):\n",
    "            img = img.to(device)\n",
    "            output = model(img).cpu().numpy()\n",
    "#             print(np.shape(output))\n",
    "            for out in output:\n",
    "                coords = extract_coords(out,logits_thresh=threshold)\n",
    "                s = coords2str(coords)\n",
    "                predictions.append(s)\n",
    "    print(np.shape(predictions))\n",
    "    test = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "    test['PredictionString']= predictions\n",
    "    file_name = \"effb0_predictions_th{}.csv\".format(threshold)\n",
    "    test.to_csv(file_name, index=False)\n",
    "    test.head()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
